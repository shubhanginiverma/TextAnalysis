{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVEOrH5wIvQk",
        "outputId": "d34c25c8-cd57-420f-fe4b-bc63ed3c401e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q xlrd\n",
        "import requests\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n"
      ],
      "metadata": {
        "id": "Nepao47z2lO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ec0dc7-8bd6-4a33-d7e9-f35a870872c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "dfUwmTqZlIun",
        "outputId": "edf0b9f7-ab7e-4443-a990-23cc0376e61b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     URL_ID                                                URL\n",
              "0      37.0  https://insights.blackcoffer.com/ai-in-healthc...\n",
              "1      38.0  https://insights.blackcoffer.com/what-if-the-c...\n",
              "2      39.0  https://insights.blackcoffer.com/what-jobs-wil...\n",
              "3      40.0  https://insights.blackcoffer.com/will-machine-...\n",
              "4      41.0  https://insights.blackcoffer.com/will-ai-repla...\n",
              "..      ...                                                ...\n",
              "109   146.0  https://insights.blackcoffer.com/blockchain-fo...\n",
              "110   147.0  https://insights.blackcoffer.com/the-future-of...\n",
              "111   148.0  https://insights.blackcoffer.com/big-data-anal...\n",
              "112   149.0  https://insights.blackcoffer.com/business-anal...\n",
              "113   150.0  https://insights.blackcoffer.com/challenges-an...\n",
              "\n",
              "[114 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f5203b1-a5d3-47ba-9c2d-15e598e1ea3a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URL_ID</th>\n",
              "      <th>URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>37.0</td>\n",
              "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38.0</td>\n",
              "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>39.0</td>\n",
              "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40.0</td>\n",
              "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>41.0</td>\n",
              "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>146.0</td>\n",
              "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>147.0</td>\n",
              "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>148.0</td>\n",
              "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>149.0</td>\n",
              "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>150.0</td>\n",
              "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>114 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f5203b1-a5d3-47ba-9c2d-15e598e1ea3a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9f5203b1-a5d3-47ba-9c2d-15e598e1ea3a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9f5203b1-a5d3-47ba-9c2d-15e598e1ea3a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# reading the excel input file into a dataframe\n",
        "df = pd.read_excel('/content/drive/MyDrive/20211030 Test Assignment/Input.xlsx')\n",
        "l = df.shape[0]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading all the stopwords into a list \n",
        "StopWordsList = []\n",
        "punc_list = [i for i in string.punctuation] # list of punctuations\n",
        "folder_path = '/content/drive/MyDrive/20211030 Test Assignment/StopWords'\n",
        "for file in os.listdir(folder_path):\n",
        "  with open(folder_path + \"/\" + file,'r', encoding = \"ISO-8859-1\") as f:\n",
        "    for line in f:\n",
        "      StopWordsList.append(line.split(None, 1)[0].lower())"
      ],
      "metadata": {
        "id": "tQw0EacBxE3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading list of positive words from the MasterDictionary\n",
        "file_pos = open('/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary/positive-words.txt','r')\n",
        "pos_words = file_pos.read().split()\n",
        "file_pos.close()\n",
        "\n",
        "# reading list of negative words from the MasterDictionary\n",
        "file_neg = open('/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary/negative-words.txt','r', encoding = \"ISO-8859-1\")\n",
        "neg_words = file_neg.read().split()\n",
        "file_neg.close()"
      ],
      "metadata": {
        "id": "qrwk-MFVx9Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to filter & count all the positive words from the article text using pos_words list\n",
        "def fn_positive_count(filtered_text):\n",
        "  pos_count = filter(lambda s : s in pos_words, filtered_text)\n",
        "  return len(list(pos_count)) # positive score\n",
        "\n",
        "# function to filter & count all the negative words from the article text using neg_words list\n",
        "def fn_negative_count(filtered_text):\n",
        "  neg_count = filter(lambda s : s in neg_words, filtered_text)\n",
        "  return len(list(neg_count)) # negative score\n",
        "\n",
        "# function to get polarity score using positive & negative score\n",
        "def fn_polarity_score(pos, neg):\n",
        "  pol_score = (pos - neg) / ((pos + neg) + 0.000001)\n",
        "  return pol_score\n",
        "\n",
        "# function to get subjectivity score using positive & negative score\n",
        "def fn_subjectivity_score(pos, neg, tot_cleaned_words):\n",
        "  subject_score = (pos + neg) / ((tot_cleaned_words) + 0.000001)\n",
        "  return subject_score\n",
        "\n",
        "# function to get syllable count of each word of article\n",
        "def fn_syllable_count(word):\n",
        "  count = 0\n",
        "  vowels = \"aeiouy\"\n",
        "  if word[0] in vowels:\n",
        "      count += 1\n",
        "  for index in range(1, len(word)):\n",
        "      if word[index] in vowels and word[index - 1] not in vowels:\n",
        "          count += 1\n",
        "  if word.endswith(\"e\"):\n",
        "      count -= 1\n",
        "  if (word.endswith(\"d\") or word.endswith(\"s\")) and word[-2] == (\"e\"): # handling -es & -ed exceptions\n",
        "    count -= 1\n",
        "  if count == 0:\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "# function to get complex word count (word with more than 2 syllables)\n",
        "def fn_complex_word_count(cleaned_text_list):\n",
        "  complex_word_count = 0\n",
        "  syllable_per_word = 0\n",
        "  total_syllable_count = 0 # number of syllables in the whole article\n",
        "  for word in cleaned_text_list: \n",
        "    syllable_per_word = fn_syllable_count(word)\n",
        "    total_syllable_count += syllable_per_word\n",
        "    if syllable_per_word > 2:\n",
        "      complex_word_count += 1\n",
        "  return complex_word_count, total_syllable_count\n",
        "\n",
        "# function to get percentage of complex words\n",
        "def fn_percent_complex(complex_words, total_words):\n",
        "  return (complex_words/total_words)\n",
        "\n",
        "# function to get fog index using average sentence length & percentage of complex words\n",
        "def fn_fog_index(avg_sentence, percent_complex):\n",
        "   fog_index = 0.4 * (avg_sentence + percent_complex)\n",
        "   return fog_index\n",
        "\n",
        "# function to get average words per sentence using total words & total sentences\n",
        "def fn_avg_words_sentence(total_words, total_sent):\n",
        "  return (total_words/total_sent)\n",
        "\n",
        "# function to get the count of all personal pronouns from the text\n",
        "def fn_personal_pronouns(text):\n",
        "  personal_pronouns = re.compile(r'\\b(I|[Ww]e|[Mm]y|[Oo]urs|[Uu]s)\\b') #using regular expression\n",
        "  pronoun_count = personal_pronouns.findall(text)\n",
        "  return len(pronoun_count)"
      ],
      "metadata": {
        "id": "l2gu87QCOeja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a dataframe to store all the variables\n",
        "output_df = pd.DataFrame(columns=[\n",
        "      'URL_ID',\n",
        "      'URL',\n",
        "      'POSITIVE SCORE',\n",
        "      'NEGATIVE SCORE',\n",
        "      'POLARITY SCORE',\n",
        "      'SUBJECTIVITY SCORE',\n",
        "      'AVG SENTENCE LENGTH',\n",
        "      'PERCENTAGE OF COMPLEX WORDS',\n",
        "      'FOG INDEX',\n",
        "      'AVG NUMBER OF WORDS PER SENTENCE',\n",
        "      'COMPLEX WORD COUNT',\n",
        "      'WORD COUNT',\n",
        "      'SYLLABLE PER WORD',\n",
        "      'PERSONAL PRONOUNS',\n",
        "      'AVG WORD LENGTH',\n",
        "    ])"
      ],
      "metadata": {
        "id": "rjFcu08IveNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# folder to store all scraped articles \n",
        "folder_path = \"articles/\"\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "idx = 0\n",
        "# for loop to iterate through all the url articles \n",
        "for i in range(0,l):\n",
        "   url = df.iloc[i][\"URL\"] # getting all urls \n",
        "   url_id = str(int(df.iloc[i][\"URL_ID\"]))\n",
        "\n",
        "   # using beautifulSoup to scrape the title and body of articles\n",
        "   headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
        "   }\n",
        "   htmlContent = requests.get(url, headers=headers).text\n",
        "   soup = BeautifulSoup(htmlContent, 'lxml')\n",
        "   # since there were 3 articles (44, 57 and 144) missing, i used try & catch block to skip the articles that are not available on website (404 error) \n",
        "   try:\n",
        "     title = soup.find('h1', class_ = 'entry-title').text \n",
        "     body = soup.find('div', class_ = 'td-post-content').text\n",
        "   except AttributeError:\n",
        "     print(\"404 Article Not Found.\")\n",
        "   else:\n",
        "     file_path = folder_path + url_id\n",
        "     # writing the title in each file\n",
        "     file1 = open(file_path,\"w+\")\n",
        "     file1.writelines(title)\n",
        "     file1.close()\n",
        "     # appending the body to the file \n",
        "     file1 = open(file_path,\"a+\")\n",
        "     file1.writelines(body)\n",
        "     file1.close()\n",
        "     # open text file in read mode\n",
        "     file1 = open(file_path, \"r\")\n",
        "     # reading the text from the file\n",
        "     data = file1.read() \n",
        "     file1.close()\n",
        "\n",
        "     # using nltk word and sentence tokenizer to get tokens of word & sentences in lists\n",
        "     tokens = word_tokenize(data)\n",
        "     sent_tokens = sent_tokenize(data) # sentence tokenize\n",
        "\n",
        "     # converting each word token into lowercase\n",
        "     for pos in range(0, len(tokens)):\n",
        "       tokens[pos] = tokens[pos].lower()\n",
        "\n",
        "     # removing all stopwords from the word tokens list to get filtered text\n",
        "     filtered_text = [t for t in tokens if not t in StopWordsList]\n",
        "\n",
        "     # removing all punctuations using filter to get cleaned text \n",
        "     cleaned_text = filter(lambda i: i not in punc_list, filtered_text)\n",
        "     cleaned_text_list = list(cleaned_text) \n",
        "     total_words = len(cleaned_text_list)\n",
        "     \n",
        "     # getting total character length of each word from the cleaned text list\n",
        "     total_char_length = 0\n",
        "     for word in cleaned_text_list:\n",
        "       char = len(word)\n",
        "       total_char_length += char\n",
        "\n",
        "     pos_count = fn_positive_count(filtered_text) \n",
        "\n",
        "     neg_count = fn_negative_count(filtered_text)\n",
        "\n",
        "     polarity = fn_polarity_score(pos_count, neg_count)\n",
        "\n",
        "     subjectivity = fn_subjectivity_score(pos_count, neg_count, total_words)\n",
        "\n",
        "     avg_sentence_len = (total_words / len(sent_tokens))\n",
        "\n",
        "     average_words_per_sent = (total_words / len(sent_tokens))\n",
        "\n",
        "     complex_word_count, total_syllable_count = fn_complex_word_count(cleaned_text_list)\n",
        "     \n",
        "     percent_complex_count = fn_percent_complex(complex_word_count, total_words)\n",
        "     \n",
        "     fog_index = fn_fog_index(avg_sentence_len, percent_complex_count)\n",
        "     \n",
        "     personal_pronoun_count = fn_personal_pronouns(data)\n",
        "     \n",
        "     avg_syllable_count =  total_syllable_count / total_words\n",
        "     \n",
        "     avg_word_length = total_char_length / total_words\n",
        "\n",
        "     # filling in the dataframe with all the output variables\n",
        "     output_df.loc[idx] = pd.Series({\n",
        "      'URL_ID': url_id,\n",
        "      'URL': url,\n",
        "      'POSITIVE SCORE': pos_count,\n",
        "      'NEGATIVE SCORE': neg_count,\n",
        "      'POLARITY SCORE': polarity,\n",
        "      'SUBJECTIVITY SCORE': subjectivity,\n",
        "      'AVG SENTENCE LENGTH': avg_sentence_len,\n",
        "      'PERCENTAGE OF COMPLEX WORDS': percent_complex_count,\n",
        "      'FOG INDEX': fog_index,\n",
        "      'AVG NUMBER OF WORDS PER SENTENCE': average_words_per_sent,\n",
        "      'COMPLEX WORD COUNT': complex_word_count,\n",
        "      'WORD COUNT': total_words,\n",
        "      'SYLLABLE PER WORD': avg_syllable_count,\n",
        "      'PERSONAL PRONOUNS': personal_pronoun_count,\n",
        "      'AVG WORD LENGTH': avg_word_length\n",
        "      })\n",
        "     idx = idx + 1\n",
        "\n",
        "# print(output_df)\n",
        "output_df.to_excel('Output Data Structure.xlsx', index=False)\n",
        "# files.download('Output Data Structure.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "CiVxtQM6EdsI",
        "outputId": "68def208-83a4-4428-b75c-10eed19970cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "404 Article Not Found.\n",
            "404 Article Not Found.\n",
            "404 Article Not Found.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5f85fc5f-6024-4d2f-bf74-4db3aecb5825\", \"Output Data Structure.xlsx\", 22317)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}